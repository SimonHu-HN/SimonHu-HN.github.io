<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hello World</title>
      <link href="/2020/05/13/hello-world/"/>
      <url>/2020/05/13/hello-world/</url>
      
        <content type="html"><![CDATA[<h2 id="初来乍到"><a href="#初来乍到" class="headerlink" title="初来乍到"></a>初来乍到</h2><h3 id="这个博客"><a href="#这个博客" class="headerlink" title="这个博客"></a>这个博客</h3>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Test</title>
      <link href="/2020/05/11/Test-1/"/>
      <url>/2020/05/11/Test-1/</url>
      
        <content type="html"><![CDATA[<p>InfluxDB (Version 1.8)</p><h2 id="源码文件目录结构：分析注解"><a href="#源码文件目录结构：分析注解" class="headerlink" title="源码文件目录结构：分析注解"></a>源码文件目录结构：分析注解</h2><h3 id="总体树形图"><a href="#总体树形图" class="headerlink" title="总体树形图"></a>总体树形图</h3><p>一级展开模块结构如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">influxdb</span><br><span class="line"></span><br><span class="line">├── client</span><br><span class="line">├── cmd</span><br><span class="line">├── coordinator</span><br><span class="line">├── etc</span><br><span class="line">├── importer</span><br><span class="line">├── influxql</span><br><span class="line">├── internal</span><br><span class="line">├── man</span><br><span class="line">├── models</span><br><span class="line">├── monitor</span><br><span class="line">├── pkg</span><br><span class="line">├── scripts</span><br><span class="line">├── services</span><br><span class="line">├── stress</span><br><span class="line">├── tcp</span><br><span class="line">├── tests</span><br><span class="line">├── toml</span><br><span class="line">├── tsdb</span><br></pre></td></tr></table></figure><p>二级展开模块结构如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── client</span><br><span class="line">│   └── v2</span><br><span class="line">├── cmd</span><br><span class="line">│   ├── influx</span><br><span class="line">│   ├── influxd</span><br><span class="line">│   ├── influx_inspect</span><br><span class="line">│   ├── influx_stress</span><br><span class="line">│   └── influx_tsm</span><br><span class="line">├── coordinator</span><br><span class="line">├── etc</span><br><span class="line">│   └── burn-in</span><br><span class="line">├── importer</span><br><span class="line">│   └── v8</span><br><span class="line">├── influxql</span><br><span class="line">│   ├── internal</span><br><span class="line">│   └── neldermead</span><br><span class="line">├── internal</span><br><span class="line">├── man</span><br><span class="line">├── models</span><br><span class="line">├── monitor</span><br><span class="line">│   └── diagnostics</span><br><span class="line">├── pkg</span><br><span class="line">│   ├── deep</span><br><span class="line">│   ├── escape</span><br><span class="line">│   ├── limiter</span><br><span class="line">│   ├── pool</span><br><span class="line">│   └── slices</span><br><span class="line">├── scripts</span><br><span class="line">├── services</span><br><span class="line">│   ├── admin</span><br><span class="line">│   ├── collectd</span><br><span class="line">│   ├── continuous_querier</span><br><span class="line">│   ├── graphite</span><br><span class="line">│   ├── httpd</span><br><span class="line">│   ├── meta</span><br><span class="line">│   ├── opentsdb</span><br><span class="line">│   ├── precreator</span><br><span class="line">│   ├── retention</span><br><span class="line">│   ├── snapshotter</span><br><span class="line">│   ├── subscriber</span><br><span class="line">│   └── udp</span><br><span class="line">├── stress</span><br><span class="line">│   ├── stress_test_server</span><br><span class="line">│   └── v2</span><br><span class="line">├── tcp</span><br><span class="line">├── tests</span><br><span class="line">│   ├── siege</span><br><span class="line">│   ├── tmux</span><br><span class="line">│   └── urlgen</span><br><span class="line">├── toml</span><br><span class="line">├── tsdb</span><br><span class="line">│   ├── engine</span><br><span class="line">│   └── internal</span><br><span class="line">└── uuid</span><br></pre></td></tr></table></figure><h3 id="重点目录注释"><a href="#重点目录注释" class="headerlink" title="重点目录注释"></a>重点目录注释</h3><h4 id="·-client"><a href="#·-client" class="headerlink" title="· client"></a>· client</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">This package provides convenience functions to read and write time series data. It uses the HTTP protocol to communicate with your InfluxDB cluster.</span><br><span class="line"></span><br><span class="line">client模块主要是在里面写了一些接口调用的语句样例，同时该模块还包含了V2子文件夹，V2包含了最新版本的client使用样例与已经写好的test文件，其不具有向前兼容性。</span><br></pre></td></tr></table></figure><p>此模块完整<a href="https://github.com/influxdata/influxdb/blob/1.8/client/README.md" target="_blank" rel="noopener">README</a>。</p><hr><h4 id="·-cmd"><a href="#·-cmd" class="headerlink" title="· cmd"></a>· cmd</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">InfluxDB相关程序所在目录。其中：</span><br><span class="line"></span><br><span class="line">influxd目录为InfluxDB主程序代码；</span><br><span class="line">influxd是一个启动influx服务的常用命令，在该子模块下存有许多子模块，其中run子模块文件的代码就是很多我们在开启influxdb之后所看到的各种参数加载过程</span><br><span class="line"></span><br><span class="line">influx为InfluxDB自带的控制台管理工具源码；</span><br><span class="line">也即是CLI</span><br><span class="line"></span><br><span class="line">influx_inspect为InfluxDB数据查看工具源码；</span><br><span class="line"></span><br><span class="line">influx_stress为InfluxDB压力测试工具源码；</span><br><span class="line"></span><br><span class="line">influx_tsm为数据库转换工具（将数据库从b1或bz1格式转换为tsm1格式）源码</span><br></pre></td></tr></table></figure><hr><h4 id="·-coordinator"><a href="#·-coordinator" class="headerlink" title="· coordinator"></a>· coordinator</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">协调器，负责数据的写入和一些创建语句的执行。</span><br><span class="line"></span><br><span class="line">在InfluxDB的ChangeLog中显示在v1.0.0中使用coordinator替换cluster。</span><br><span class="line"></span><br><span class="line">coordinator是tsdb的一个封装，原本是提供分布式存储、保证数据一致性等功能，但是现在由于集群功能不再开源，其中 这部分逻辑也已经被移除，现在仅仅是tsdb的代理。</span><br></pre></td></tr></table></figure><hr><h4 id="·-etc"><a href="#·-etc" class="headerlink" title="· etc"></a>· etc</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">存放一些默认的influxdb配置，包含证书认证等路径的值的配置</span><br></pre></td></tr></table></figure><hr><h4 id="·-importer"><a href="#·-importer" class="headerlink" title="· importer"></a>· importer</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">数据导入相关包，直观点可以这样理解，当使用chronograf时在explorer处会有一个import，而importer包定义了数据导入的具体需求格式。</span><br><span class="line"></span><br><span class="line">官方也对数据导入也有一个示范</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/image-20200421175342935.png" alt="image-20200421195259133"></p><img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/image-20200421164354001.png" alt="image-20200421175342935" style="zoom: 67%;" /><hr><h4 id="·-internal"><a href="#·-internal" class="headerlink" title="· internal"></a>· internal</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">主要实现了MetaClient接口</span><br></pre></td></tr></table></figure><hr><h4 id="·-man"><a href="#·-man" class="headerlink" title="· man"></a>· man</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">帮助手册</span><br></pre></td></tr></table></figure><hr><h4 id="·-logger"><a href="#·-logger" class="headerlink" title="· logger"></a>· logger</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">记录日志模块</span><br></pre></td></tr></table></figure><hr><h4 id="·-pkg"><a href="#·-pkg" class="headerlink" title="· pkg"></a>· pkg</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">一些通用包的集合。</span><br><span class="line"></span><br><span class="line">deep里面主要实现了deepValueEqual方法，用于深层次比较两个值是否相等；</span><br><span class="line"></span><br><span class="line">escape里面主要实现了byte和string两种数据类型转义字符的相关操作；</span><br><span class="line"></span><br><span class="line">limiter里面主要是一个基于channel实现的简单并发限制器Fixed；</span><br><span class="line"></span><br><span class="line">pool里面主要实现了Bytes和Generic两种类型的Pool，在Pool中的对象不使用时不会被垃圾回收自动清理掉；</span><br><span class="line"></span><br><span class="line">slices 里面主要实现了一些string数组的操作；</span><br></pre></td></tr></table></figure><hr><h4 id="·-scripts"><a href="#·-scripts" class="headerlink" title="· scripts"></a>· scripts</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">该目录存放的是一些关于InfluxDB的脚本。</span><br></pre></td></tr></table></figure><hr><h4 id="·-service"><a href="#·-service" class="headerlink" title="· service"></a>· service</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">collectd：</span><br><span class="line">#############################</span><br><span class="line">为collectd（https:&#x2F;&#x2F;collectd.org）对接服务，可以接收通过UDP发送过来的collectd格式数据；</span><br><span class="line"></span><br><span class="line">graphite：</span><br><span class="line">#############################</span><br><span class="line">为InfluxDB的graphite服务；</span><br><span class="line"></span><br><span class="line">opentsdb：</span><br><span class="line">#############################</span><br><span class="line">为InfluxDB的opentsdb服务，可用于替换opentsdb；</span><br><span class="line"></span><br><span class="line">influxdb兼容了主流的多种监控数据接收协议有graphit&#x2F;opentsdb&#x2F;collectd。这些模块接收到相关数据后，将数据 解析成统一格式Point 然后通过coordinator.PointsWriter.WritePointsPrivileged() &#x2F;coordinator.PointsWriter.WritePoints() &#x2F;coordinator.PointsWriter.WritePointsInto() 写入tsdb。</span><br><span class="line"></span><br><span class="line">continuous_querier：</span><br><span class="line">#############################</span><br><span class="line">为InfluxDB的CQ服务；</span><br><span class="line"></span><br><span class="line">httpd：</span><br><span class="line">#############################</span><br><span class="line">是influxdb对外提供服务的主要协议，用户可以通过http协议查询数据、数据写入、修改database、增加用户等操作。对外 提供的了一些API 和&#x2F;debug&#x2F;pprof、&#x2F;debug&#x2F;expvar、&#x2F;debug&#x2F;requests接口。</span><br><span class="line"></span><br><span class="line">meta：</span><br><span class="line">#############################</span><br><span class="line">为InfluxDB的元数据服务，用于管理数据库的元数据相关内容；influxdb将meta数据库，包装成一个MetaClient对外提供数据，需要meta的模块都引用这个MetaCient。该meta.db 直接使用protobuf格式的数据作为持久化文件。meta加载持久化文件后，会将全部内容缓存在内存中。当有meta改写时， MetaClient会将更新后的数据序列化然后写入磁盘中。MetaClient一部分数据已slice的形式存储，很多api都会将该slice返回给调用方，从而脱离了其锁的保护，有数据并发竞争 访问的问题存在。meta.db中存储每个database的元数据(名称、过期策略、ContinuousQuery)和用户信息。</span><br><span class="line"></span><br><span class="line">precreator：</span><br><span class="line">#############################</span><br><span class="line">为InfluxDB的Shard预创建服务；influxd将时序数据按时间区间来切分文件，被切分的块称之为shard。当influxdb组成一个集群时，shard写时创建 可能会在触发切换文件的一瞬间影响进程的吞吐量，因此采用预先创建shard的方式，使得文件总是预先被创建的，写请求 不会被切换文件的动作阻塞。</span><br><span class="line">该模块就是创建一个定时函数，每隔一定时间间隔，检查并按需预先创建一次shard。</span><br><span class="line"></span><br><span class="line">retention:</span><br><span class="line">#############################</span><br><span class="line">为InfluxDB的数据保留策略的强制执行服务，主要用于定时删除文件；influxdb通常存储一些监控、统计数据，通常来说，这些数据并不需要一直存储着，当数据超过一定期限时，这些数据 就失去了保留的意义。同时influxdb的存储空间也不是无限的。因此influxdb就内置了数据过期自动删除的逻辑。对于 influxdb上的每个database可以设置一些过期策略，满足这些策略时，influxdb就是删除满足条件的shard。</span><br><span class="line">retention就是执行这些过期策略的模块。该模块定期检查每个database的过期策略，将满足过期条件的ShardGroup删除， 然后再将与该ShardGroup相关的shard删除。</span><br><span class="line"></span><br><span class="line">snapshotter:</span><br><span class="line">#############################</span><br><span class="line">为InfluxDB的快照服务；</span><br><span class="line"></span><br><span class="line">subscriber:</span><br><span class="line">#############################</span><br><span class="line">为InfluxDB的订阅服务；subscriber处理influxdb中SUBSCRIPTIONS部分的逻辑，将内部通过coordinator.PointsWriter写入的数据发送到 database中配置的SUBSCRIPTIONS指定的URI上，采用推的模式。有点类似Watch的机制。Kapacitor 就使用了此机制。各个数据接收模块都会使用到coordinator.PointsWriter。</span><br><span class="line"></span><br><span class="line">udp:</span><br><span class="line">#############################</span><br><span class="line">为InfluxDB的udp服务，可以通过该接口进行数据库的写入和查询等操作；</span><br></pre></td></tr></table></figure><hr><h4 id="·-stress"><a href="#·-stress" class="headerlink" title="· stress"></a>· stress</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">该目录存放的是压力测试相关内容。</span><br></pre></td></tr></table></figure><hr><h4 id="·-tcp"><a href="#·-tcp" class="headerlink" title="· tcp"></a>· tcp</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">网络连接的多路复用。</span><br></pre></td></tr></table></figure><hr><h4 id="·-tests"><a href="#·-tests" class="headerlink" title="· tests"></a>· tests</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">测试相关内容</span><br></pre></td></tr></table></figure><hr><h4 id="·-toml"><a href="#·-toml" class="headerlink" title="· toml"></a>· toml</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">toml的解析器，和另一个toml解析器(github.com&#x2F;BurntSushi&#x2F;toml)不同，为独立的解析模块，主要是解析时间字符串和磁盘容量数据。</span><br></pre></td></tr></table></figure><hr><h4 id="·-tsdb"><a href="#·-tsdb" class="headerlink" title="· tsdb"></a>· tsdb</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tsdb目录主要是时序数据库的实现。</span><br></pre></td></tr></table></figure><hr><h4 id="·-uuid"><a href="#·-uuid" class="headerlink" title="· uuid"></a>· uuid</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">该目录里面主要存放uuid生成的相关代码。</span><br></pre></td></tr></table></figure><hr><h2 id="运行时相关配置路径、端口、文件结构"><a href="#运行时相关配置路径、端口、文件结构" class="headerlink" title="运行时相关配置路径、端口、文件结构"></a>运行时相关配置路径、端口、文件结构</h2><p>端口：</p><ul><li><p>8086 数据保存、查询端口，单机</p></li><li><p>8083 influxdb的web查询界面，单机</p></li><li><p>8088 集群使用</p></li></ul><p>配置文件</p><ul><li>/etc/influxdb/influxdb.conf 默认的配置文件 修改连接参数，重启即可</li><li>/var/log/influxdb/influxd.log 日志文件</li><li>/var/lib/influxdb/data 数据文件</li><li>/usr/lib/influxdb/scripts 初始化脚本文件夹</li><li>/usr/bin/influx 启动数据库</li><li>/var/run/influxdb/influxd.pid 服务启动的进程id</li><li>/var/cache/yum/influxdb 缓存处理数据</li></ul><p>文件结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">├── data  #</span><br><span class="line">│   ├── _internal</span><br><span class="line">│   │   └── monitor</span><br><span class="line">│   │       ├── 73</span><br><span class="line">│   │       │   └── 000000003-000000002.tsm</span><br><span class="line">│   │       ├── 74</span><br><span class="line">│   │       │   └── 000000003-000000002.tsm</span><br><span class="line">│   │       └── 75</span><br><span class="line">│   │           └── 000000001-000000001.tsm</span><br><span class="line">│   └── testing</span><br><span class="line">│       └── autogen</span><br><span class="line">│           └── 2</span><br><span class="line">│               └── 000000002-000000002.tsm</span><br><span class="line">├── meta</span><br><span class="line">│   └── meta.db</span><br><span class="line">└── wal# 配置文件[data]下wal-dir配置</span><br><span class="line">    ├── _internal</span><br><span class="line">    │   └── monitor</span><br><span class="line">    │       ├── 73</span><br><span class="line">    │       │   └── _00012.wal</span><br><span class="line">    │       ├── 74</span><br><span class="line">    │       │   └── _00012.wal</span><br><span class="line">    │       └── 75</span><br><span class="line">    │           ├── _00005.wal</span><br><span class="line">    │           ├── _00006.wal</span><br><span class="line">    │           └── _00007.wal</span><br><span class="line">    └── testing</span><br><span class="line">        └── autogen</span><br><span class="line">            └── 2</span><br><span class="line">                └── _00003.wal</span><br></pre></td></tr></table></figure><hr><h2 id="操作流程原理分析与记录"><a href="#操作流程原理分析与记录" class="headerlink" title="操作流程原理分析与记录"></a>操作流程原理分析与记录</h2><h3 id="启动过程"><a href="#启动过程" class="headerlink" title="启动过程"></a>启动过程</h3><ul><li><p>Influxdb的启动入口实现在 <code>cmd/influxd/main.go</code>中</p><p>The <code>influxd</code> command line interface (CLI) starts and runs all the processes necessary for InfluxDB to function.</p></li><li><p><code>influxd</code> 支持下列启动命令（run -config 是比较常用的类型)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">backup               downloads a snapshot of a data node and saves it to disk</span><br><span class="line">config               display the default configuration</span><br><span class="line">help                 display this help message</span><br><span class="line">restore              uses a snapshot of a data node to rebuild a cluster</span><br><span class="line">run                  run node with existing configuration</span><br><span class="line">version              displays the InfluxDB version</span><br></pre></td></tr></table></figure></li></ul><p><code>Run命令代码</code></p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *Main)</span> <span class="title">Run</span><span class="params">(args ...<span class="keyword">string</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    name, args := cmd.ParseCommandName(args)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Extract name from args.</span></span><br><span class="line">    <span class="keyword">switch</span> name &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">""</span>, <span class="string">"run"</span>:</span><br><span class="line">        cmd := run.NewCommand()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Tell the server the build details.</span></span><br><span class="line">        cmd.Version = version</span><br><span class="line">        cmd.Commit = commit</span><br><span class="line">        cmd.Branch = branch</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> err := cmd.Run(args...); err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> fmt.Errorf(<span class="string">"run: %s"</span>, err)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        signalCh := <span class="built_in">make</span>(<span class="keyword">chan</span> os.Signal, <span class="number">1</span>)</span><br><span class="line">        signal.Notify(signalCh, os.Interrupt, syscall.SIGTERM)</span><br><span class="line">        cmd.Logger.Info(<span class="string">"Listening for signals"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Block until one of the signals above is received</span></span><br><span class="line">        &lt;-signalCh</span><br><span class="line">        cmd.Logger.Info(<span class="string">"Signal received, initializing clean shutdown..."</span>)</span><br><span class="line">        <span class="keyword">go</span> cmd.Close()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Block again until another signal is received, a shutdown timeout elapses,</span></span><br><span class="line">        <span class="comment">// or the Command is gracefully closed</span></span><br><span class="line">        cmd.Logger.Info(<span class="string">"Waiting for clean shutdown..."</span>)</span><br><span class="line">        <span class="keyword">select</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> &lt;-signalCh:</span><br><span class="line">            cmd.Logger.Info(<span class="string">"Second signal received, initializing hard shutdown"</span>)</span><br><span class="line">        <span class="keyword">case</span> &lt;-time.After(time.Second * <span class="number">30</span>):</span><br><span class="line">            cmd.Logger.Info(<span class="string">"Time limit reached, initializing hard shutdown"</span>)</span><br><span class="line">        <span class="keyword">case</span> &lt;-cmd.Closed:</span><br><span class="line">            cmd.Logger.Info(<span class="string">"Server shutdown completed"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>主要就是<code>cmd := run.NewCommand()</code>创建cmd对象，然后调用其<code>Run</code>方法</p><p><code>Command.Run</code>的实现</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(cmd *Command)</span> <span class="title">Run</span><span class="params">(args ...<span class="keyword">string</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    <span class="comment">// 解析参数</span></span><br><span class="line">    options, err := cmd.ParseFlags(args...)</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 解析配置文件，初始化各组件的配置信息</span></span><br><span class="line">    config, err := cmd.ParseConfig(options.GetConfigPath())</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化logger</span></span><br><span class="line">    <span class="keyword">if</span> cmd.Logger, logErr = config.Logging.New(cmd.Stderr); logErr != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="comment">// assign the default logger</span></span><br><span class="line">        cmd.Logger = logger.New(cmd.Stderr)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果配置了pid file path, 就写pud</span></span><br><span class="line">    cmd.writePIDFile(options.PIDFile)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建Server对象，并调用Open方法将 Server运行起来</span></span><br><span class="line">    s, err := NewServer(config, buildInfo)</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">if</span> err := s.Open(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> fmt.Errorf(<span class="string">"open server: %s"</span>, err)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 开如monitor server error信息</span></span><br><span class="line">    <span class="keyword">go</span> cmd.monitorServerErrors()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>再分析<code>NewServer(config, buildInfo)</code>部分，它主要的功能就是依据配置Server对象和它管理的各个组件, 主要包括</p><ul><li>Monitor</li><li>MetaClient</li><li>TSDBStore</li><li>Subscriber</li><li>PoinitsWriter</li><li>QueryExecutor<br>…</li></ul><p>在此之后会调用<code>Server.Open</code>添加各种service，使各个service active</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Open opens the meta and data store and all services.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *Server)</span> <span class="title">Open</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    <span class="comment">// 创建并运行一个tcp的连接复用器</span></span><br><span class="line">    ln, err := net.Listen(<span class="string">"tcp"</span>, s.BindAddress)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> fmt.Errorf(<span class="string">"listen: %s"</span>, err)</span><br><span class="line">    &#125;</span><br><span class="line">    s.Listener = ln</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Multiplex listener.</span></span><br><span class="line">    mux := tcp.NewMux()</span><br><span class="line">    <span class="keyword">go</span> mux.Serve(ln)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 添加各种service</span></span><br><span class="line">    s.appendMonitorService()  <span class="comment">//</span></span><br><span class="line">    s.appendPrecreatorService(s.config.Precreator) <span class="comment">//预创建ShardGroup</span></span><br><span class="line">    s.appendSnapshotterService() <span class="comment">//使用上面的tcp连接复用器，处理snapshot相关的请求</span></span><br><span class="line">    s.appendContinuousQueryService(s.config.ContinuousQuery) <span class="comment">// 连续query服务</span></span><br><span class="line">    s.appendHTTPDService(s.config.HTTPD) <span class="comment">//http服务，接收并处理所有客户端的请求</span></span><br><span class="line">    s.appendRetentionPolicyService(s.config.Retention) <span class="comment">//依据RetentionPolicy周期性的作清理</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Graphite, Collectd, OpenTSDB都会对其实TSDB数据格式的支持</span></span><br><span class="line">    <span class="keyword">for</span> _, i := <span class="keyword">range</span> s.config.GraphiteInputs &#123;</span><br><span class="line">        <span class="keyword">if</span> err := s.appendGraphiteService(i); err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> err</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> _, i := <span class="keyword">range</span> s.config.CollectdInputs &#123;</span><br><span class="line">        s.appendCollectdService(i)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> _, i := <span class="keyword">range</span> s.config.OpenTSDBInputs &#123;</span><br><span class="line">        <span class="keyword">if</span> err := s.appendOpenTSDBService(i); err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> err</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> _, i := <span class="keyword">range</span> s.config.UDPInputs &#123;</span><br><span class="line">        s.appendUDPService(i)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Open TSDB store.</span></span><br><span class="line">    <span class="keyword">if</span> err := s.TSDBStore.Open(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> fmt.Errorf(<span class="string">"open tsdb store: %s"</span>, err)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Open the subscriber service</span></span><br><span class="line">    <span class="keyword">if</span> err := s.Subscriber.Open(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> fmt.Errorf(<span class="string">"open subscriber: %s"</span>, err)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Open the points writer service</span></span><br><span class="line">    <span class="keyword">if</span> err := s.PointsWriter.Open(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> fmt.Errorf(<span class="string">"open points writer: %s"</span>, err)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    s.PointsWriter.AddWriteSubscriber(s.Subscriber.Points())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _, service := <span class="keyword">range</span> s.Services &#123;</span><br><span class="line">        <span class="keyword">if</span> err := service.Open(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> fmt.Errorf(<span class="string">"open service: %s"</span>, err)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>启动流程总览图</p><p><img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/image-20200421192448162.png" alt="image-20200421192448162"></p><hr><h3 id="写入过程"><a href="#写入过程" class="headerlink" title="写入过程"></a>写入过程</h3><img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/155128070_2_20190226050052739.jpg" alt="155128070_2_20190226050052739" style="zoom: 50%;" /><p><img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/20191105195100456.png" alt="20191105195100456"></p><ul><li>Client: 这边的Client指能与influxDB服务器交互的客户端，如influx，influx-java，chronograf等。</li><li>Server<ul><li>Httpd: 接收处理http请求。InfluxDB是RESTful服务器，所有的客户端请求都会被封装成http请求发送给服务器端进行处理</li><li>Influxql: 用于解析SQL，生成AST等</li><li>Coordinator：协调模块</li><li>StatementExecutor: 将SQL statement发送给Meta和Store模块进行处理</li><li>PointsWriter：将实际数据写入交由Store模块进行处理</li><li>Store: Shard管理模块</li><li>Shard: 实际写入查询模块</li><li>Engine：执行引擎</li><li>Index: 倒排索引管理模块</li></ul></li></ul><p><strong>具体流程</strong></p><ol><li>client发送写入请求给server端</li><li>httpd模块接收到写入请求，从http body中提取写入的数据并解析成point（注意tagValue中“\n”换行符需要转义，否则会将\n后的内容当成下一个point进行解析）</li><li>pointsWriter模块接收到httpd模块解析的points后与meta模块进行交互获得points -&gt; shardGroup之间的映射关系，如下图所示：</li></ol><p>​                                  <img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/image-20200421201150774.png" alt="image-20200421201150774" style="zoom: 50%;" /></p><p>每个point都会有相应的时间戳，根据时间戳会划分到不同的shard group（每个shard group根据retention policy配置的shard duration管理相应长度的时间区间），每个shard group中有多个shard（单机版中只有1个shard），shard是InfluxDB中数据分片管理基本单元，待写入的points会根据SeriesKey hash到不同的shard中交由shard模块进行写入</p><ol start="4"><li><p>shard模块进行point数据的写入分为两部分：（1）构建索引（2）时序数据的写入：</p><p><strong>构建索引：Index子模块负责构建索引。（相关概念：TSI文件、磁盘索引倒排)</strong></p></li></ol><ul><li><p>首先会提取出写入point的<code>SeriesKey</code>，将<code>SeriesKey</code>进行hash从<code>SeriesIndex</code>文件的<code>KeyIDMapBlock</code>对应的hash桶中得到该<code>SeriesKey</code>对应的<code>SeriesID</code>，<code>SeriesID</code>的高32位表示series segment文件id，通过高32为可以找到存储该<code>SeriesKey</code>所在的series segment文件，低32位表示在series segment文件中的偏移量，从而找到<code>SeriesID</code>对应的<code>SeriesKey</code>，如果从series segment中找到的<code>SeriesKey</code>与待写入point的<code>SeriesKey</code>相同，则该<code>SeriesKey</code>的索引已经存在，不用再新建索引，如果不相同，则需要将该新的<code>SeriesKey</code>追加写入series segment文件中（写入新的series segment entry），同时在<code>series index</code>中也插入该series segment entry的索引项，<code>series index</code>的内容暂时存放在内存中，当series index中的索引项达到128K时，series index会发生compact，将内存中的数据写入到磁盘中生成series index文件。</p></li><li><p>当有新的<code>SeriesKey</code>生成时，该<code>SeriesKey</code>对应的<code>SeriesID</code>也要写入TSL文件中（后续查询数据都是先从TSL和TSI文件中获取<code>SeriesID</code>，通过<code>SeriesID</code>在去查找对应的<code>SeriesKey</code>），当TSL文件大小达到配置项max-index-log-file-size时，TSL文件会compact成TSI文件。除此之外，还需要在保存在内存中的<code>LogFile</code>结构中构建<code>SeriesKey</code>的倒排索引，该倒排索引可以理解为形同<code>Map&lt;measurement, Map&lt;tagKey, Map&lt;tagValue, List&lt;SeriesID&gt;&gt;&gt;&gt;</code>的多级Map结构（该多级Map结构方便后续在进行查询where过滤时能够快速查到对应的<code>SeriesID</code>)</p></li><li><p>将新写入的field写入<code>fields.idx</code>文件，<code>field.idx</code>文件主要在select时判断该field是否存在以及获取field的类型</p></li></ul><p><strong>时序数据的写入：engine模块负责数据写入</strong></p><ul><li><p>将point中的<code>SeriesKey</code>和各个field提取出来组成Key(<code>SeriesKey</code> + 单个<code>field</code>）得到<code>Map&lt;Key, List&lt;FieldValue&gt;&gt;</code>，写入memory cache中（可以理解成<code>memtable</code>）</p><ul><li><p>将point插入记录写入WAL文件中</p></li><li><p>写入cache</p><ol><li><p>第一步：数据格式化。格式化为一个<code>Map&lt;key,List&lt;timeStamp|value&gt;&gt;</code></p></li><li><p>第二步：时序数据点写入cache。<code>influxDB</code>中cache是一个crude hash ring，这里的ring是由256个partition构成，每个partition负责存储一部分时序数据Key对应的值，写入数据的时候首先会根据key hash一次，hash到对应的partition。为什么这么处理，个人认为有点像Java中<code>ConcurrentHashMap</code>的思路，将一个大<code>HashMap</code>切分成多个小<code>HashMap</code>，每个<code>HashMap</code>内部在写的时候需要加锁。这样处理可以减小锁粒度，提高写性能。</p></li></ol></li><li><p>当memory cache大小达到配置文件中cache-snapshot-memory-size(默认是25M大小)或在WAL写入后经过cache-snapshot-write-cold-duration时间后(默认时间阈值是10分钟)，将cache compact成TSM文件落盘</p></li><li><p>删除已经落盘的数据所对应的WAL文件</p></li></ul></li></ul><h4 id="批量数据写入流程："><a href="#批量数据写入流程：" class="headerlink" title="批量数据写入流程："></a>批量数据写入流程：</h4><p>批量数据进入到<code>influxDB</code>之后总体会经过三个步骤的处理：</p><ol><li>批量时序数据shard路由：<code>influxDB</code>首先会将这些数据根据shard的不同分成不同的分组，每个分组的时序数据会发送到对应的shard。</li><li>倒排索引引擎构建倒排索引：<code>InfluxDB</code>中shard由两个LSM引擎构成：倒排索引引擎和TSM引擎。时序数据首先会经过倒排索引引擎构建倒排索引。</li><li>TSM引擎持久化时序数据：倒排索引构建成功之后时序数据会进入TSM Engine。TSM Engine处理流程：先将写入请求追加写入WAL日志，再写入cache，满足特定条件就将cache中的时序数据执行flush操作落盘形成TSM File</li></ol><h3 id="查询流程"><a href="#查询流程" class="headerlink" title="查询流程"></a>查询流程</h3><p>在处理查询时：</p><ol><li>首先，通过 TSI 查询符合条件的 SeriesID</li><li>再从 SeriesFile 当中找到对应的 SeriesKey</li><li>找到 SeriesKey 之后，再筛选出包含 SeriesKey 数据的 TSM 文件（一般还会根据时间范围进行筛选，过滤掉时间范围不符合条件的文件）</li><li>最后，合并 TSM 文件中的数据，用于做进一步的计算（window、aggregation 等等）</li></ol><p>时序数据最常见的查询场景有两个：按时间维度查询 和按数据源维度查询。如何提高这两个场景的查询效率是每种时序数据库都要充分考虑的问题。</p><p>结合之前的文章中所提到的数据写入流程和存储结构，我们很容易了解到InfluxDB中，如果能根据查询条件确定数据所属的时间范围和<code>series</code>（即所属数据源的唯一标识），即可大量减少数据扫描的范围，从而提高性能。InfluxDB正是通过这样的方式来提高查询数据的效率。</p><p>假设在如下图的场景中，需要查询<code>atlalsdata</code>发布在baidu的所有广告2018年8月份的pc端的总点击量：</p><img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/20180808143157448.png" alt="20180808143157448" style="zoom: 50%;" /><p>查询语句:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">sum</span>(pc) <span class="keyword">from</span> advertise</span><br><span class="line"><span class="keyword">where</span> publisher=<span class="string">'atlasdata'</span> <span class="keyword">and</span> platform = <span class="string">'baidu'</span></span><br><span class="line"><span class="keyword">and</span> <span class="built_in">time</span>&gt;=<span class="string">'2018-08-01'</span> <span class="keyword">and</span> <span class="built_in">time</span> &lt;<span class="string">'2018-09-01'</span></span><br></pre></td></tr></table></figure><p>数据读取的流程：</p><p>第一步：找出atlasdata发布在baidu的所有广告的<code>seriesKey</code></p><p>第二步：根据<code>seriesKey</code>找到这些广告2018年8月份的数据</p><ul><li>通过时间范围确定存储2018年8月份数据的<code>shardgroup</code>, 再根据<code>seriesKey</code>进行hash找到<code>shardgroup</code>下存储这些数据源的shard</li></ul><p>第三步：找出这些数据中的pc端的数据</p><ul><li>扫描每个shard中的TSM文件中的index block，找到符合条件的data block，再在data block中把符合条件的所查询的数据筛选出来进行聚合</li></ul><p>查询流程中的第二步和第三步通过简单的说明都很好理解，而第一步是IinfluxDB通过对tags创建倒排索引来实现的。</p><h4 id="详细说明"><a href="#详细说明" class="headerlink" title="详细说明"></a><strong>详细说明</strong></h4><h5 id="Part-I：InfluxDB数据读取流程原理"><a href="#Part-I：InfluxDB数据读取流程原理" class="headerlink" title="Part I：InfluxDB数据读取流程原理"></a>Part I：InfluxDB数据读取流程原理</h5><p>LSM(TSM)引擎对于读流程的处理通常来说都比较复杂，建议保持足够的耐心和专注力。理论部分会分两个小模块进行介绍，第一个模块会从宏观框架层面简单梳理整个读取流程，第二个模块会从微观细节层面分析TSM存储引擎（TSDB）内部详细的执行逻辑。</p><p><strong>InfluxDB读取流程框架</strong></p><p>笔者对照源码对整个流程做了一个简单的梳理（1.6版本，非1.8）：</p><p><img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/2020022531.png" alt="2020022531"></p><p>整个读取流程从宏观上分为四个部分：</p><ol><li><p>Query：InfluxQL允许用户使用类SQL语句执行查询分析聚合，InfluxQL语法详见：</p></li><li><p>QueryParser：InfluxQL进入系统之后，系统首先会对InfluxQL执行切词并解析为抽象语法树（AST），抽象树中标示出了数据源、查询条件、查询列以及聚合函数等等，分别对应上图中Source、Condition以及Aggration。InfluxQL没有使用通用的第三方AST解析库，自己实现了一套解析库，对细节感兴趣的可以参考：<a href="https://github.com/influxdata/influxql。接着InfluxDB会将抽象树转化为一个Query实体对象，供后续查询中使用。" target="_blank" rel="noopener">https://github.com/influxdata/influxql。接着InfluxDB会将抽象树转化为一个Query实体对象，供后续查询中使用。</a></p></li><li><p>BuildIterators：InfluxQL语句转换为Query实体对象之后，就进入读取流程中最重要最核心的一个环节 – 构建Iterator体系。构建Iterator体系是一个非常复杂的逻辑过程，其中细节非常繁复，笔者尽可能化繁为简，将其中的主线抽出来。为了方便理解，笔者将Iterator体系分为三个子体系：顶层Iterator子体系、中间层Iterator子体系以及底层Iterator子体系。</p><ul><li><p>（1）顶层Iterator子体系：</p><p>InfluxDB会为InfluxQL中所有查询field构造一个FieldIterator，FieldIterator表示每个查询列都会创建一个Iterator（称为ExprIterator），这是因为InfluxDB是列式存储系统，所有的列都是独立存储的，因此基于列分别构建Iterator方便执行查询聚合操作。比如sum(click)，sum(impressions)和sum(revenue)三个查询列就分别对应一个ExprIterator。</p><p>ExprIterator根据查询列值是否需要聚合可以分为VarRefIterator和CallIterator，前者表示列值可以直接查询返回，不需要聚合；后者表示查询列需要执行某些聚合操作。示例中查询sum(click)就是典型的CallIterator，CallIterator实际实现分为两步，首先通过VarRefIterator把对应的列值查询到，再通过对应的Reduce函数执行相应聚合。比如sum(click)这个CallIterator就需要雇佣一个VarRefIterator把满足条件的click列值拿上来，再执行Reduce函数sum执行聚合操作。</p></li><li><p>（2）中间层Iterator子体系：</p><p>InfluxDB中一个查询列的值可能分布在不同的Shard上，需要根据TimeRange决定给定时间段在哪些shard上，并为每个Shard构建一个Iterator，雇佣这个逻辑Iterator负责查询这个shard上对应列的列值。目前单机版所有shard都在同一个InfluxDB实例上，如果实现分布式管理，需要在这一层做处理。</p></li><li><p>（3）底层Iterator子体系：</p><p>底层Iterator子体系负责单个shard(engine)上满足条件的某一列值的查找或者单机聚合，是Iterator体系中实际干活的Iterator。比如满足where advertiser = “baidu.com” 这个条件就需要先在倒排索引中根据advertiser = “baidu.com”查到包含该tag的所有series，再为每个series构建一个TagsetIterator去查找对应的列值，TagsetIterator会将查找指针置于最小的列值处。</p><p>纵观整个Iterator体系的构建，整体逻辑还是很清晰的。总结起来就是，查询按照查询列构建最顶层FieldIterator，每个FieldIterator会根据TimeRange雇佣多个ShardIterator去处理单个Shard上面对应列值的查找，对查找到的值要么直接返回要么执行Reduce函数进行聚合操作。每个Shard内部首先会根据查询条件利用倒排索引定位到所有满足条件的series，再为每个series构建一个TagsetIterator用来查找具体的列值数据。因此，TagsetIterator是整个体系中唯一干活的Iterator，所有其他上层Iterator都是逻辑Iterator。</p><p>另一个非常重要的点是，同一个Shard内的所有TagsetIterator在构建完成会合并成一个ShardIterator，这个合并过程是对这些TagsetIterator进行排序的过程，排序规则是按照series由小到大排序或者由大到小排序（由用户SQL对查询结果是由小到大排序还是由大到小排序决定）。同理，一个列值对应的多个ShardIterator构建完成之后会合并成一个FieldIterator，合并过程亦是一个排序过程，不过排序是针对所有Shard中的TagsetIterator进行的，排序规则是先比较series，再比较时间。可见，一个FieldIterator最终是由一系列排序过的TagsetIterator构成的。</p></li></ul></li><li><p>Emitter.Emit：Iterator体系构建完成之后就完成了查询聚合前的准备工作，接下来就开始干活了。干活逻辑简单来讲是遍历所有FieldIterator，对每个FieldIterator执行一次Next函数，就会返回每个查询列的结果值，组装到一起就是一行数据。FieldIterator执行Next()函数会传递到最底层的TagsetIterator，TagsetIterator执行Next函数实际返回真实的时序数据。</p></li></ol><p><strong>TSDB存储引擎执行逻辑</strong></p><p>TSDB存储引擎（实际上就是一个Shard）根据用户的查询请求执行原始数据的查询就是上文中提到的底层Iterator子体系的构建。查询过程分为两个部分：倒排索引查询过滤以及TSM数据层查询，前者通过Query中的where条件结合倒排索引过滤掉不满足条件的SeriesKey；后者根据留下的SeriesKey以及where条件中时间段信息（TimeRange）在TSMFile中以及内存中查出最终满足条件的数值列。TSDB存储引擎会将查询到的所有满足条件的原始数值列返回给上层，上层根据聚合函数对原始数据进行聚合并将聚合结果返回给用户。整个过程如下图所示：</p><p><img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/2020022532.png" alt="2020022532"></p><p>上图需要从底部向上浏览，整个流程可以整理为如下：</p><ol><li><p>根据where condition以及所有倒排索引文件查处所有满足条件的SeriesKey</p></li><li><p>将满足条件的SeriesKey根据GroupBy维度列进行分组，不同分组后续的所有操作都可以独立并发执行，因此可以多线程处理</p></li><li><p>针对某个分组的SeriesKey集合以及待查询列，根据指定查询时间段（TimeRange）在所有TSMFile中根据B+树索引构建查询iterator</p></li><li><p>将满足条件的原始数据返回给上层进行聚合运算，并将聚合运算的结果返回给用户</p></li></ol><p>实际执行的过程可能比较抽象，为了更好的理解，笔者在下半部分举了一个示例。没有理解上面的逻辑没关系，可以先看下面的示例，看完之后再看上面的理论逻辑相信会更加容易理解。</p><h5 id="Part-II：InfluxDB查询流程示例"><a href="#Part-II：InfluxDB查询流程示例" class="headerlink" title="Part II：InfluxDB查询流程示例"></a>Part II：InfluxDB查询流程示例</h5><p>[Part I](##### Part I：InfluxDB数据读取流程原理 )从理论层面对InfluxDB查询流程进行了介绍。为了方便理解TSDB存储引擎处理查询流程的逻辑，笔者通过如下一个真实示例将其中的核心步骤进行说明。下表为原始时序数据表，表中有3个维度列：publisher、advertiser以及gender，3个数值列：impression、click以及revenue：</p><p><img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/2020022533.png" alt="2020022533"></p><p>现在用户想查询2018年1月份发布在baidu.com平台上的不同广告商的曝光量、点击量以及总收入，SQL如下所示：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">sum</span>(click),<span class="keyword">sum</span>(impression),<span class="keyword">sum</span>(revenue) <span class="keyword">from</span> <span class="keyword">table</span> <span class="keyword">group</span> <span class="keyword">by</span> publisher <span class="keyword">where</span> advertiser = <span class="string">"baidu.com"</span> <span class="keyword">and</span> <span class="built_in">timestamp</span> &gt; <span class="string">"2018-01-01"</span> <span class="keyword">and</span> <span class="built_in">timestamp</span> &lt; <span class="string">"2018-02-01"</span></span><br></pre></td></tr></table></figure><p>步骤一：倒排索引过滤+groupby分组</p><p>原始查询语句：select …. from ad_datasource where advertiser = “baidu.com” …… 。倒排索引即根据条件advertiser=”baidu.com”在所有Index File中遍历查询包含该tag的所有SeriesKey，如下：</p><ol><li><p>根据Index File中Measurement Block根据”ad_datasource”进行过滤，可以直接定位到给定source对应的所有TagKey所在的文件offset|size。</p></li><li><p>加载出对应TagKey区域的Hash Index，使用给定TagKey（”advertiser”）进行hash可以直接定位到该TagKey对应的TagValue的文件offset|size。</p></li><li><p>加载出TagKey对应TagValue区域的Hash Index，使用过滤条件TagValue（”baidu.com”）进行hash可以直接定位到该TagValue对应的所有SeriesID。</p></li><li><p>SeriesID就是对应SeriesKey在索引文件中的offset，直接根据SeriesID可以加载出对应的SeriesKey。</p></li></ol><p>满足条件的所有SeriesKey如下表所示，共有3个：</p><p><img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/2020022534.png" alt="2020022534"></p><p>根据倒排索引查询得到所有的SeriesKey之后，这里有一个非常重要的步骤：根据groupby条件对SeriesKey进行分组，分组算法为hash。示例查询中聚合条件为group by publisher，因此需要将上面得到的3个SeriesKey按照publisher的不同分成如下两组：</p><p><img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/2020022535.png" alt="2020022535"></p><p>在倒排索引之后执行分组意义非常重大，分组后不同group的SeriesKey是可以并行独立执行查询并最终执行聚合的，因此后续的所有操作都可以使用多个线程并发执行，极大提升整个查询性能。</p><p>步骤二：TSM文件数据检索</p><p>到这一步，我们已经按照groupby得到分组后的SeriesKey集合。接下来需要根据SeriesKey以及TimeRange在TSM数据文件中查找满足条件的待查询列。在TSM数据文件中根据SeriesKey以及TimeRange查询field的具体过程如下：</p><p><img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/2020022536.jpg" alt="2020022536"></p><p>上图中中间部分为索引层，TSM在启动之后就会将TSM文件的索引部分加载到内存，数据部分因为太大并不会直接加载到内存。用户查询可以分为三步：</p><ol><li><p>首先根据Key（SeriesKey+fieldKey）找到对应的SeriesIndex Block，因为Key是有序的，所以可以使用二分查找来具体实现</p></li><li><p>找到SeriesIndex Block之后再根据查找的时间范围，使用[MinTime, MaxTime]索引定位到可能的Series Data Block列表</p></li><li><p>将满足条件的Series Data Block加载到内存中解压进一步使用二分查找算法查找即可找到</p></li></ol><p>在TSM中查询满足TimeRange条件的SeriesKey对应的待查询列值，因为InfluxDB会根据不同的查询列设置独立的FieldIterator，因此查询列有多少就有多少个FieldIterator，如下所示：</p><p><img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/2020022537.png" alt="2020022537"></p><p>步骤三：原始数据聚合</p><p>查询到满足条件的所有原始数据之后，InfluxDB会根据查询聚合函数对原始数据进行聚合，如下图所示：</p><p><img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/final.png" alt="final"></p><p>文章总结</p><p>本文主要结合InfluxDB源码对查询聚合请求在服务器端的处理框架进行了系统理论介绍，同时深入介绍了InfluxDB Shard Engine是如何利用倒排索引、时序数据存储文件（TSMFile）处理用户的查询请求。最后，举了一个示例对Shard Engine的执行流程进行了形象化说明。</p><hr><h2 id="专业术语相关概念"><a href="#专业术语相关概念" class="headerlink" title="专业术语相关概念"></a>专业术语相关概念</h2><h3 id="关键保留字"><a href="#关键保留字" class="headerlink" title="关键保留字"></a>关键保留字</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">ALL           ALTER         ANY           AS            ASC           BEGIN</span><br><span class="line">BY            CREATE        CONTINUOUS    DATABASE      DATABASES     DEFAULT</span><br><span class="line">DELETE        DESC          DESTINATIONS  DIAGNOSTICS   DISTINCT      DROP</span><br><span class="line">DURATION      END           EVERY         EXPLAIN       FIELD         FOR</span><br><span class="line">FROM          GRANT         GRANTS        GROUP         GROUPS        IN</span><br><span class="line">INF           INSERT        INTO          KEY           KEYS          KILL</span><br><span class="line">LIMIT         SHOW          MEASUREMENT   MEASUREMENTS  NAME          OFFSET</span><br><span class="line">ON            ORDER         PASSWORD      POLICY        POLICIES      PRIVILEGES</span><br><span class="line">QUERIES       QUERY         READ          REPLICATION   RESAMPLE      RETENTION</span><br><span class="line">REVOKE        SELECT        SERIES        SET           SHARD         SHARDS</span><br><span class="line">SLIMIT        SOFFSET       STATS         SUBSCRIPTION  SUBSCRIPTIONS TAG</span><br><span class="line">TO            USER          USERS         VALUES        WHERE         WITH</span><br><span class="line">WRITE</span><br></pre></td></tr></table></figure><p>The keyword <code>time</code> is a special case. <code>time</code> can be a <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#continuous-query-cq" target="_blank" rel="noopener">continuous query</a> name, database name, <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#measurement" target="_blank" rel="noopener">measurement</a> name, <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#retention-policy-rp" target="_blank" rel="noopener">retention policy</a> name, <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#subscription" target="_blank" rel="noopener">subscription</a> name, and <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#user" target="_blank" rel="noopener">user</a> name. In those cases, <code>time</code> does not require double quotes in queries. <code>time</code> cannot be a <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#field-key" target="_blank" rel="noopener">field key</a> or <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#tag-key" target="_blank" rel="noopener">tag key</a>; InfluxDB rejects writes with <code>time</code> as a field key or tag key and returns an error. （time关键字的使用需要注意)</p><h3 id="TSM"><a href="#TSM" class="headerlink" title="TSM"></a>TSM</h3><p>官方文档：</p><p><a href="https://docs.influxdata.com/influxdb/v1.8/concepts/storage_engine/#storage-engine" target="_blank" rel="noopener">In-memory indexing and the Time-Structured Merge Tree (TSM) | InfluxData Documentation</a></p><hr><h3 id="TSI"><a href="#TSI" class="headerlink" title="TSI"></a>TSI</h3><p>官方文档：</p><p><a href="https://docs.influxdata.com/influxdb/v1.8/concepts/tsi-details" target="_blank" rel="noopener">Time Series Index (TSI) details | InfluxData Documentation</a><br><a href="https://docs.influxdata.com/influxdb/v1.8/concepts/time-series-index/" target="_blank" rel="noopener">Time Series Index (TSI) overview | InfluxData Documentation</a></p><h4 id="1-基于内存的索引倒排序"><a href="#1-基于内存的索引倒排序" class="headerlink" title="1. 基于内存的索引倒排序"></a>1. 基于内存的索引倒排序</h4><p><strong>作用：</strong></p><p>提高数据源维度的查询效率：找出符合查询条件的所有数据源的<code>seriesKey</code></p><p><strong>依赖的核心数据结构：</strong></p><ol><li><p><strong>·<code>map&lt;tagkey, map&lt;tagvalue, List&lt;SeriesID&gt;&gt;&gt;</code></strong> : 保存每个tag对应的所有值对应的所有<code>seriesID</code>，通过条件（比如 tag1=’value’）获取满足条件的所有数据源的唯一标识</p><ul><li><code>tagkey</code>: 数据源属性名称</li><li><code>tagvalue</code>: <code>tagkey</code>所指属性的值</li></ul></li></ol><ul><li><code>SeriesID</code>：满足<code>tagkey=tagvalue</code>的所有数据源的唯一标识，<code>seriesID</code>与数据源唯一标识<code>SeriesKey</code>一一对应</li></ul><ol start="2"><li><p><strong>·<code>map&lt;SeriesID,SeriesKey&gt;</code></strong> : 保存<code>seriesID</code>和<code>SeriesKey</code>的映射关系，通过<code>seriesID</code>获取<code>seriesKey</code></p><p>FAQ:  为什么第一个双重map中不直接用<code>SeriesKey</code>呢？</p><p>之所以引入 SeriesID，我想是为了提高内存使用效率。试想，如果直接索引 SeriesKey 的话，SeriesKey 是一个 byte 数组，格式上有个多个 tagKey 和 tagValue 拼接而成，是很占内存的。而 SeriesID 是自增的 64 位整形，索引 SeriesID 的话内存占用最少会降低一个数量级，通过位图可以进一步提高内存使用效率。</p><p>不过，增加了 SeriesID，就必须要增加 SeriesID 和 SeriesKey 映射关系的存储，也就增加了系统的复杂度。是为了节省内存空间，</p><p>举个例子：</p><p>假如现在有3个<code>Tag</code>组合形成一个<code>seriesKey</code>：<code>measurement=m_name,tag_1=value_1,tag_2=value_2,tag_3=value_3</code>。那么构造形成的双重map结构:</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tag_1, &lt;value_1, seriesKey&gt;&gt;</span><br><span class="line">&lt;tag_2, &lt;value_2, seriesKey&gt;&gt;</span><br><span class="line">&lt;tag_3, &lt;value_3, seriesKey&gt;&gt;</span><br></pre></td></tr></table></figure><p>可以发现同一个<code>seriesKey</code>在内存中有多份冗余，而且<code>seriesKey</code> = measurement + tags，随着tag值的增加，内存消耗非常明显，因此为了节省内存消耗，采用<code>Int</code>对<code>seriesKey</code>进行编码，代替双重map中的<code>seriesKey</code></p></li></ol><p><strong>优缺点</strong></p><ul><li>优点</li></ul><p>​    加快数据源维度的查询效率</p><ul><li>缺点</li></ul><ol><li>受限于内存大小</li><li>一旦InfluxDB进程宕掉，需要扫描解析所有TSM文件并在内存中重新构建，恢复时间很长。</li></ol><h4 id="2-基于磁盘的倒排索引-TSI文件"><a href="#2-基于磁盘的倒排索引-TSI文件" class="headerlink" title="2. 基于磁盘的倒排索引-TSI文件"></a>2. 基于磁盘的倒排索引-TSI文件</h4><p><strong>介绍：</strong></p><p>When InfluxDB ingests data, we store not only the value but we also index the measurement and tag information so that it can be queried quickly. In earlier versions, index data could only be stored in-memory, however, that requires a lot of RAM and places an upper bound on the number of series a machine can hold. This upper bound is usually somewhere between 1 - 4 million series depending on the machine used.</p><p>The Time Series Index (TSI) was developed to allow us to go past that upper bound. TSI stores index data on disk so that we are no longer restricted by RAM. <u>TSI uses the operating system’s page cache to pull hot data into memory and let cold data rest on disk.</u></p><p><strong>作用：</strong></p><p>提高数据源维度的查询效率：找出符合查询条件的所有数据源的seriesKey，并解决内存索引受限于内存大小、恢复时间长的问题。</p><h5 id="摘自官网："><a href="#摘自官网：" class="headerlink" title="摘自官网："></a>摘自官网：</h5><p><strong>结构</strong>：</p><p>TSI (Time Series Index) is a log-structured merge tree-based database for InfluxDB series data. TSI is composed of several parts:</p><ul><li><strong>Index</strong>: Contains the entire index dataset for a single shard.</li><li><strong>Partition</strong>: Contains a sharded partition of the data for a shard.</li><li><strong>LogFile</strong>: Contains newly written series as an in-memory index and is persisted as a WAL.</li><li><strong>IndexFile</strong>: Contains an immutable, memory-mapped index built from a LogFile or merged from two contiguous index files.</li></ul><p>There is also a <strong>SeriesFile</strong> which contains a set of all series keys across the entire database. Each shard within the database shares the same series file.</p><p><strong>Writes</strong> （写入过程时产生的与TSI相关的流程)</p><p>The following occurs when a write comes into the system:</p><ol><li>Series is added to the series file or is looked up if it already exists. This returns an auto-incrementing series ID.</li><li>The series is sent to the Index. The index maintains a roaring bitmap of existing series IDs and ignores series that have already been created.</li><li>The series is hashed and sent to the appropriate Partition.</li><li>The Partition writes the series as an entry to the LogFile.</li><li>The LogFile writes the series to a write-ahead log file on disk and adds the series to a set of in-memory indexes.</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Note:</span><br><span class="line"></span><br><span class="line">官网此处的Series：[measurement + tag set + retention policy] 与中文许多博客教程上的SeriesKey为同义，而一个 series ID 单独对应一个 seriesKey。在后面的过程中，都是通过series ID来寻找seriesKey的，从而实现检索；</span><br><span class="line"></span><br><span class="line">此处的Partition 它包含层级结构的index files和一个LogFile,其中这个层级结构的index files就是L1-l7的tsi（类比LSM-Tree）,这个LogFile就是tsl(它算作是L0)，在磁盘上的目录结构上，它位于每个shard目录下。一个partiton下包含有一个tsl文件（.tsl files are &quot;time series log&quot; files that are part of the TSI indexing process.），若干tsi文件和一个MANIFEST文件。</span><br><span class="line"></span><br><span class="line">tsl：就是WAL,新写入的index信息除了在内存里缓存外，还会以LogEntry的形式写入这个tsl，作故障恢复时用。</span><br><span class="line">L0层LogFile会定期compact到L1, L1-L6会定期向高层作compact， compact的过程其实就是将相同measurement的tagbock作在一起，相同measurement的相同tagkey对应的所有tagvalue放在一起， 相同measurement的相同tagkey又相同tagvalue的不同series id作合并后放在一起。</span><br></pre></td></tr></table></figure><p><strong>Compaction</strong>（合并）</p><p>Once the LogFile exceeds a threshold (5MB), then a new active log file is created and the previous one begins compacting into an IndexFile. This first index file is at level 1 (L1). The log file is considered level 0 (L0).</p><p>Index files can also be created by merging two smaller index files together. For example, if contiguous two L1 index files exist then they can be merged into an L2 index file.</p><p>Compact方法源码:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *Partition)</span> <span class="title">compact</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">if</span> p.isClosing() &#123;</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> !p.compactionsEnabled() &#123;</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    interrupt := p.compactionInterrupt</span><br><span class="line"></span><br><span class="line">    fs := p.retainFileSet()</span><br><span class="line">    <span class="keyword">defer</span> fs.Release()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//influxdb的每个partition中tsi的层次是固定的L0-L7,其中L0是wal,这个方法不涉及它的compact</span></span><br><span class="line">    <span class="comment">//L7为最高层，它也不会再被compact了</span></span><br><span class="line">    <span class="comment">//所以这个compact方法需要处理的是L1-L6层</span></span><br><span class="line">    minLevel, maxLevel := <span class="number">1</span>, <span class="built_in">len</span>(p.levels)<span class="number">-2</span></span><br><span class="line">    <span class="keyword">for</span> level := minLevel; level &lt;= maxLevel; level++ &#123;</span><br><span class="line">        <span class="comment">//如果正在被compact则跳过</span></span><br><span class="line">        <span class="keyword">if</span> p.levelCompacting[level] &#123;</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取当前level中的相邻的index文件列表，按文件更改时间由新到旧排,每次最多compact两个文件，少于两个的不作compact</span></span><br><span class="line">        files := fs.LastContiguousIndexFilesByLevel(level)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(files) &lt; <span class="number">2</span> &#123;</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> <span class="built_in">len</span>(files) &gt; MaxIndexMergeCount &#123;</span><br><span class="line">            files = files[<span class="built_in">len</span>(files)-MaxIndexMergeCount:]</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Retain files during compaction.</span></span><br><span class="line">        IndexFiles(files).Retain()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Mark the level as compacting.</span></span><br><span class="line">        p.levelCompacting[level] = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 开goroutine作compact</span></span><br><span class="line">        <span class="function"><span class="keyword">func</span><span class="params">(files []*IndexFile, level <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">            <span class="comment">// Start compacting in a separate goroutine.</span></span><br><span class="line">            p.wg.Add(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">                <span class="comment">// compact到高一级.</span></span><br><span class="line">                p.compactToLevel(files, level+<span class="number">1</span>, interrupt)</span><br><span class="line">                <span class="comment">// Ensure compaction lock for the level is released.</span></span><br><span class="line">                p.mu.Lock()</span><br><span class="line">                p.levelCompacting[level] = <span class="literal">false</span></span><br><span class="line">                p.mu.Unlock()</span><br><span class="line">                p.wg.Done()</span><br><span class="line">                <span class="comment">// Check for new compactions</span></span><br><span class="line">                p.Compact()</span><br><span class="line">            &#125;()</span><br><span class="line">        &#125;(files, level)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>Additional：<a href="https://www.jianshu.com/p/5c846e205f5f" target="_blank" rel="noopener">LSM-tree </a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Note:</span><br><span class="line">这里就是上面一个Note的官方说法，也即是LogFile为L0层，Index file逐层向上合并</span><br></pre></td></tr></table></figure><p><strong>Reads</strong>（读取操作）</p><p>The index provides several API calls for retrieving sets of data such as:</p><ul><li><code>MeasurementIterator()</code>: Returns a sorted list of measurement names.</li><li><code>TagKeyIterator()</code>: Returns a sorted list of tag keys in a measurement.</li><li><code>TagValueIterator()</code>: Returns a sorted list of tag values for a tag key.</li><li><code>MeasurementSeriesIDIterator()</code>: Returns a sorted list of all series IDs for a measurement.</li><li><code>TagKeySeriesIDIterator()</code>: Returns a sorted list of all series IDs for a tag key.</li><li><code>TagValueSeriesIDIterator()</code>: Returns a sorted list of all series IDs for a tag value.</li></ul><p>These iterators are all composable using several merge iterators. For each type of iterator (measurement, tag key, tag value, series id), there are multiple merge iterator types:</p><ul><li><strong>Merge</strong>: Deduplicates items from two iterators.</li><li><strong>Intersect</strong>: Returns only items that exist in two iterators.</li><li><strong>Difference</strong>: Only returns items from first iterator that don’t exist in the second iterator.</li></ul><p>For example, a query with a WHERE clause of <code>region != &#39;us-west&#39;</code> that operates across two shards will construct a set of iterators like this:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DifferenceSeriesIDIterators(</span><br><span class="line">    MergeSeriesIDIterators(</span><br><span class="line">        Shard1.MeasurementSeriesIDIterator(<span class="string">"m"</span>),</span><br><span class="line">        Shard2.MeasurementSeriesIDIterator(<span class="string">"m"</span>),</span><br><span class="line">    ),</span><br><span class="line">    MergeSeriesIDIterators(</span><br><span class="line">        Shard1.TagValueSeriesIDIterator(<span class="string">"m"</span>, <span class="string">"region"</span>, <span class="string">"us-west"</span>),</span><br><span class="line">        Shard2.TagValueSeriesIDIterator(<span class="string">"m"</span>, <span class="string">"region"</span>, <span class="string">"us-west"</span>),</span><br><span class="line">    ),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h6 id="关键文件结构"><a href="#关键文件结构" class="headerlink" title="关键文件结构"></a>关键文件结构</h6><p><strong>Log File Structure</strong></p><p>The log file is simply structured as a list of LogEntry objects written to disk in sequential order. Log files are written until they reach 5MB and then they are compacted into index files. The entry objects in the log can be of any of the following types:</p><ul><li>AddSeries</li><li>DeleteSeries</li><li>DeleteMeasurement</li><li>DeleteTagKey</li><li>DeleteTagValue</li></ul><p>The in-memory index on the log file tracks the following:</p><ul><li>Measurements by name</li><li>Tag keys by measurement</li><li>Tag values by tag key</li><li>Series by measurement</li><li>Series by tag value （tag value的series）</li><li>Tombstones for series, measurements, tag keys, and tag values.</li></ul><p>The log file also maintains bitsets for series ID existence and tombstones. These bitsets are merged with other log files and index files to regenerate the full index bitset on startup.</p><p><strong>Index File Structure</strong></p><p>The index file is an immutable file that tracks similar information to the log file, but all data is indexed and written to disk so that it can be directly accessed from a memory-map.</p><p>（index file是一个不变的文件，它会追踪log file中相似的信息，但是所有的数据都被索引且写入磁盘，以便于可以通过内存映射来直接访问。）</p><p>The index file has the following sections:</p><ul><li><strong>TagBlocks:</strong> Maintains an index of tag values for a single tag key. （为单个tag key维护一个tag value的索引）</li><li><strong>MeasurementBlock:</strong> Maintains an index of measurements and their tag keys.（维护measurements的索引以及他们的tag keys）</li><li><strong>Trailer:</strong> Stores offset information for the file as well as HyperLogLog sketches for cardinality estimation. （为所有的文件存储偏移量信息(offset)）</li></ul><p><strong>Manifest</strong></p><p>The MANIFEST file is stored in the index directory and lists all the files that belong to the index and the order in which they should be accessed. This file is updated every time a compaction occurs. Any files that are in the directory that are not in the index file are index files that are in the process of being compacted.</p><p><strong>FileSet</strong></p><p>A file set is an in-memory snapshot of the manifest that is obtained while the InfluxDB process is running. This is required to provide a consistent view of the index at a point-in-time. The file set also facilitates reference counting for all of its files so that no file will be deleted via compaction until all readers of the file are done with it.</p><p><strong>FileSet</strong> 是File类型的集合，这个 File类型可能是LogFile，也可能是 IndexFile，功能与的<code>IndexFiles</code>类似，定义如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">type FileSet struct &#123;</span><br><span class="line">    levels       []CompactionLevel</span><br><span class="line">    sfile        *tsdb.SeriesFile</span><br><span class="line">    files        []File &#x2F;&#x2F; 按最后更改时间从小到大排列</span><br><span class="line">    manifestSize int64 &#x2F;&#x2F; Size of the manifest file in bytes.</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>提供了一系列的iterator操作，按measurement name来汇集了所有index文件中的measurement, tagkey, tagvalue, series id set等，且作了排序</li><li>文件替换操作， 参数中<code>oldFiles</code>是fs.files的一部分，即当前正在被compat的文件列表，这个方法的目的是将这oldFiles列表从fs.files中删除，然后在<code>oldFiles</code>原来开始的位置插入这个<code>newFile</code>, 这个<code>newFile</code>就是compact之后新生成的文件。</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(fs *FileSet)</span> <span class="title">MustReplace</span><span class="params">(oldFiles []File, newFile File)</span> *<span class="title">FileSet</span></span> &#123;</span><br><span class="line">    assert(<span class="built_in">len</span>(oldFiles) &gt; <span class="number">0</span>, <span class="string">"cannot replace empty files"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Find index of first old file.</span></span><br><span class="line">    <span class="keyword">var</span> i <span class="keyword">int</span></span><br><span class="line">    <span class="keyword">for</span> ; i &lt; <span class="built_in">len</span>(fs.files); i++ &#123;</span><br><span class="line">        <span class="keyword">if</span> fs.files[i] == oldFiles[<span class="number">0</span>] &#123;</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> i == <span class="built_in">len</span>(fs.files)<span class="number">-1</span> &#123;</span><br><span class="line">            <span class="built_in">panic</span>(<span class="string">"first replacement file not found"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Ensure all old files are contiguous.</span></span><br><span class="line">    <span class="keyword">for</span> j := <span class="keyword">range</span> oldFiles &#123;</span><br><span class="line">        <span class="keyword">if</span> fs.files[i+j] != oldFiles[j] &#123;</span><br><span class="line">            <span class="built_in">panic</span>(fmt.Sprintf(<span class="string">"cannot replace non-contiguous files: subset=%+v, fileset=%+v"</span>, Files(oldFiles).IDs(), Files(fs.files).IDs()))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Copy to new fileset.</span></span><br><span class="line">    other := <span class="built_in">make</span>([]File, <span class="built_in">len</span>(fs.files)-<span class="built_in">len</span>(oldFiles)+<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">copy</span>(other[:i], fs.files[:i])</span><br><span class="line">    other[i] = newFile</span><br><span class="line">    <span class="built_in">copy</span>(other[i+<span class="number">1</span>:], fs.files[i+<span class="built_in">len</span>(oldFiles):])</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Build new fileset and rebuild changed filters.</span></span><br><span class="line">    <span class="keyword">return</span> &amp;FileSet&#123;</span><br><span class="line">        levels: fs.levels,</span><br><span class="line">        files:  other,</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/v2-b846f02c761b11b5352546e098288033_1440w.jpg" alt="v2-b846f02c761b11b5352546e098288033_1440w" style="zoom: 67%;" /><h5 id="摘自CSDN："><a href="#摘自CSDN：" class="headerlink" title="摘自CSDN："></a>摘自CSDN：</h5><p>总体结构：</p><img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/20180809110541985.png" alt="20180809110541985" style="zoom: 67%;" /><p>Measurement Block 结构：</p><img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/image-20200422142823468.png" alt="image-20200422142823468" style="zoom: 80%;" /><p>Measurement Block由三大部分组成：</p><ul><li>Block Trailer : 记录各模块的offset和size</li><li>HashIndex : 记录每个measurement的偏移量</li><li>Mesurement : 记录具体表的信息,包括表的名称、所有tag对应的Tag Block的offset和size等</li></ul><p>Measurement Block的扫描流程如下：</p><ol><li>首先在TSI文件的Index File Trailer部分获取整个Mesurement Block的offset和size，</li><li>然后再根据Measurement Block中的Block Trailer和Hash Index确认要查询的表对应的Measurement的位置，</li><li>读取对应的Measurement中的Tag Block的offset和size</li></ol><hr><h2 id="Influx-Query-Language"><a href="#Influx-Query-Language" class="headerlink" title="Influx Query Language"></a>Influx Query Language</h2><h3 id="数据写入"><a href="#数据写入" class="headerlink" title="数据写入"></a>数据写入</h3><h4 id="写入方式"><a href="#写入方式" class="headerlink" title="写入方式"></a>写入方式</h4><p><strong>InfluxDB API</strong></p><p>Write data to InfluxDB using the InfluxDB API. Send a <code>POST</code> request to the <code>/write</code> endpoint and provide your line protocol in the request body:</p><p>官方与常见的做法是通过curl来进行数据的传递</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -i -XPOST <span class="string">"http://localhost:8086/write?db=science_is_cool"</span> --data-binary <span class="string">'weather,location=us-midwest temperature=82 1465839830100400200'</span></span><br><span class="line"></span><br><span class="line">/*不光是--data-binary 使用@filename 的方法用本地文件(内含写好的数据插入语句)作为数据也可以实现数据写入功能*/</span><br></pre></td></tr></table></figure><p>For in-depth descriptions of query string parameters, status codes, responses, and more examples, see the <a href="https://docs.influxdata.com/influxdb/v1.8/tools/api/#write-http-endpoint" target="_blank" rel="noopener">API Reference</a>.</p><p><strong>CLI</strong></p><p>Write data to InfluxDB using the InfluxDB command line interface (CLI). <a href="https://docs.influxdata.com/influxdb/v1.8/tools/shell/#launch-influx" target="_blank" rel="noopener">Launch</a> the CLI, use the relevant database, and put <code>INSERT</code> in front of your line protocol:</p><p>首先开启influxd，随后在开启influx，这就成功开启了CLI，可以进行操作了。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> weather,location=us-midwest temperature=<span class="number">82</span> <span class="number">1465839830100400200</span></span><br></pre></td></tr></table></figure><p>You can also use the CLI to <a href="https://docs.influxdata.com/influxdb/v1.8/tools/shell/#import-data-from-a-file-with-import" target="_blank" rel="noopener">import</a> Line Protocol from a file.</p><p>There are several ways to write data to InfluxDB. See the <a href="https://docs.influxdata.com/influxdb/v1.8/tools/" target="_blank" rel="noopener">Tools</a> section for more on the <a href="https://docs.influxdata.com/influxdb/v1.8/tools/api/#write-http-endpoint" target="_blank" rel="noopener">InfluxDB API</a>, the <a href="https://docs.influxdata.com/influxdb/v1.8/tools/shell/" target="_blank" rel="noopener">CLI</a>, and the available Service Plugins ( <a href="https://docs.influxdata.com/influxdb/v1.8/tools/udp/" target="_blank" rel="noopener">UDP</a>, <a href="https://docs.influxdata.com/influxdb/v1.8/tools/graphite/" target="_blank" rel="noopener">Graphite</a>, <a href="https://docs.influxdata.com/influxdb/v1.8/tools/collectd/" target="_blank" rel="noopener">CollectD</a>, and <a href="https://docs.influxdata.com/influxdb/v1.8/tools/opentsdb/" target="_blank" rel="noopener">OpenTSDB</a>).</p><h4 id="语法规则"><a href="#语法规则" class="headerlink" title="语法规则"></a>语法规则</h4><p>InfluxDB line protocol is a text based format for writing points to InfluxDB.</p><p><strong>Line protocol syntax</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;measurement&gt;[,&lt;tag_key&gt;=&lt;tag_value&gt;[,&lt;tag_key&gt;=&lt;tag_value&gt;]] &lt;field_key&gt;=&lt;field_value&gt;[,&lt;field_key&gt;=&lt;field_value&gt;] [&lt;timestamp&gt;]</span><br></pre></td></tr></table></figure><p>Line protocol accepts the newline character <code>\n</code> and is whitespace-sensitive.</p><blockquote><p><strong>Note</strong> Line protocol does not support the newline character <code>\n</code> in tag values or field values.</p></blockquote><p>Syntax description</p><p>InfluxDB line protocol informs InfluxDB of the data’s measurement, tag set, field set, and timestamp.</p><table><thead><tr><th>Element</th><th>Optional/Required</th><th>Description</th><th>Type (See <a href="https://docs.influxdata.com/influxdb/v1.8/write_protocols/line_protocol_reference#data-types" target="_blank" rel="noopener">data types</a> for more information.)</th></tr></thead><tbody><tr><td><a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#measurement" target="_blank" rel="noopener">Measurement</a></td><td>Required</td><td>The measurement name. InfluxDB accepts one measurement per point.</td><td>String</td></tr><tr><td><a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#tag-set" target="_blank" rel="noopener">Tag set</a></td><td>Optional</td><td>All tag key-value pairs for the point.</td><td><a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#tag-key" target="_blank" rel="noopener">Tag keys</a> and <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#tag-value" target="_blank" rel="noopener">tag values</a> are both strings.</td></tr><tr><td><a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#field-set" target="_blank" rel="noopener">Field set</a></td><td>Required. Points must have at least one field.</td><td>All field key-value pairs for the point.</td><td><a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#field-key" target="_blank" rel="noopener">Field keys</a> are strings. <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#field-value" target="_blank" rel="noopener">Field values</a> can be floats, integers, strings, or Booleans.</td></tr><tr><td><a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#timestamp" target="_blank" rel="noopener">Timestamp</a></td><td>Optional. InfluxDB uses the server’s local nanosecond timestamp in UTC if the timestamp is not included with the point.</td><td>The timestamp for the data point. InfluxDB accepts one timestamp per point.</td><td>Unix nanosecond timestamp. Specify alternative precisions with the <a href="https://docs.influxdata.com/influxdb/v1.8/tools/api/#write-http-endpoint" target="_blank" rel="noopener">InfluxDB API</a>.</td></tr></tbody></table><blockquote><h4 id="Performance-tips"><a href="#Performance-tips" class="headerlink" title="Performance tips:"></a><a href="https://docs.influxdata.com/influxdb/v1.8/write_protocols/line_protocol_reference#performance-tips" target="_blank" rel="noopener">Performance tips:</a></h4><ul><li>Before sending data to InfluxDB, sort by tag key to match the results from the <a href="http://golang.org/pkg/bytes/#Compare" target="_blank" rel="noopener">Go bytes.Compare function</a>.</li><li>To significantly improve compression, use the coarsest <a href="https://docs.influxdata.com/influxdb/v1.8/tools/api/#write-http-endpoint" target="_blank" rel="noopener">precision</a> possible for timestamps.</li><li>Use the Network Time Protocol (NTP) to synchronize time between hosts. InfluxDB uses a host’s local time in UTC to assign timestamps to data. If a host’s clock isn’t synchronized with NTP, the data that the host writes to InfluxDB may have inaccurate timestamps.</li></ul></blockquote><p>不同类型数据的写入实例</p><p><strong>In a measurement, a field’s type cannot differ in a <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#shard" target="_blank" rel="noopener">shard</a>, but can differ across shards.</strong></p><ul><li>科学记数法方式的数值写入</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; INSERT mymeas value=-1.234456e+78</span><br></pre></td></tr></table></figure><ul><li>float类型数值写入</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; INSERT mymeas value=1.0</span><br><span class="line"></span><br><span class="line">or</span><br><span class="line"></span><br><span class="line">&gt; INSERT mymeas value=1.0</span><br></pre></td></tr></table></figure><ul><li>int类型数值写入</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; INSERT mymeas value=1i   /*i = int */</span><br></pre></td></tr></table></figure><ul><li>String类型的写入</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; INSERT mymeas value="stringing along"</span><br></pre></td></tr></table></figure><ul><li>Boolean类型数值写入</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; INSERT mymeas value=true</span><br></pre></td></tr></table></figure><p><strong>Attempt to write a string to a field that previously accepted floats</strong></p><p>If the timestamps on the float and string are stored in the same shard: （在同一个shard下，同一个field名称下的值的数据类型必须相同，不然就会冲突）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; INSERT mymeas value=3 1465934559000000000 /*value 为 float型 */</span><br><span class="line">&gt; INSERT mymeas value="stringing along" 1465934559000000001 /*value 为 String型 */</span><br><span class="line">ERR: &#123;"error":"field type conflict: input field \"value\" on measurement \"mymeas\" is type string, already exists as type float"&#125;</span><br></pre></td></tr></table></figure><p>If the timestamps on the float and string are not stored in the same shard: （非同一个shard下存储的则数据类型可以不同）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; INSERT mymeas value=3 1465934559000000000 /*value 为 float型 */</span><br><span class="line">&gt; INSERT mymeas value="stringing along" 1466625759000000000 /*value 为 String型 */</span><br></pre></td></tr></table></figure><h4 id="特殊符号"><a href="#特殊符号" class="headerlink" title="特殊符号"></a><strong>特殊符号</strong></h4><p>引号的使用：</p><p><strong>Quoting</strong></p><table><thead><tr><th>Element</th><th>Double quotes</th><th>Single quotes</th></tr></thead><tbody><tr><td>Timestamp</td><td>Never</td><td>Never</td></tr><tr><td>Measurements, tag keys, tag values, field keys</td><td>Never*</td><td>Never*</td></tr><tr><td>Field values</td><td>Double quote string field values. Do not double quote floats, integers, or Booleans.</td><td>Never</td></tr></tbody></table><blockquote><p>* InfluxDB line protocol allows users to double and single quote measurement names, tag keys, tag values, and field keys. It will, however, assume that the double or single quotes are part of the name, key, or value. This can complicate query syntax (see the example below).</p><p>不要把表的名字用引号包裹起来，这会增加语法的复杂性</p></blockquote><ul><li><p>Never single quote field values (even if they’re strings!). It’s also not valid line protocol. (单引号有毒)</p><p>Example:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; INSERT weather,location&#x3D;us-midwest temperature&#x3D;&#39;too warm&#39;</span><br><span class="line">ERR: &#123;&quot;error&quot;:&quot;unable to parse &#39;weather,location&#x3D;us-midwest temperature&#x3D;&#39;too warm&#39;&#39;: invalid boolean&quot;&#125;</span><br></pre></td></tr></table></figure></li><li><p>Do not double or single quote measurement names, tag keys, tag values, and field keys. It is valid line protocol but InfluxDB assumes that the quotes are part of the name.(单引号有毒)</p><p>Example:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt; INSERT weather,location&#x3D;us-midwest temperature&#x3D;82 1465839830100400200</span><br><span class="line">&gt; INSERT &quot;weather&quot;,location&#x3D;us-midwest temperature&#x3D;87 1465839830100400200</span><br><span class="line">&gt; SHOW MEASUREMENTS</span><br><span class="line">name: measurements</span><br><span class="line">------------------</span><br><span class="line">name</span><br><span class="line">&quot;weather&quot;</span><br><span class="line">weather</span><br></pre></td></tr></table></figure></li></ul><p><strong>Emoji</strong></p><p>All other special characters also do not require escaping. For example, line protocol handles emojis with no problem:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; INSERT we⛅️ther,location=us-midwest temper🔥ture=82 1465839830100400200</span><br><span class="line">&gt; SELECT * FROM "we⛅️ther"</span><br><span class="line">name: we⛅️ther</span><br><span class="line"><span class="comment">------------------</span></span><br><span class="line">time      location    temper🔥ture</span><br><span class="line">1465839830100400200   us-midwest 82</span><br></pre></td></tr></table></figure><p><strong>中文</strong></p><p>根据实地测试结果，中文字符在CLI写入方式下，可以正常的作为measurements, tag key/value, field key/value写入到数据库中，并且在select后可以正常显示。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&gt; insert china,姓名=<span class="string">'赵'</span> 年龄=11</span><br><span class="line">&gt; select * from china</span><br><span class="line">name: china</span><br><span class="line">time                姓名     年龄</span><br><span class="line">----                ------ ------</span><br><span class="line">1587715020915798534 <span class="string">'胡'</span>    11</span><br><span class="line">1587715087762670335 <span class="string">'赵'</span>    11</span><br><span class="line">1587717954983126804 <span class="string">'赵'</span>    11</span><br><span class="line"></span><br><span class="line">—————————————————————————————————————————————————————————————————————————————————————————————————————</span><br><span class="line"></span><br><span class="line">&gt; insert 测试,数据点=桩1 情况=<span class="string">"良好"</span></span><br><span class="line">&gt; select * from 测试</span><br><span class="line">ERR: error parsing query: found 测, expected identifier at line 1, char 15</span><br><span class="line">&gt; show measurements</span><br><span class="line">name: measurements</span><br><span class="line">name</span><br><span class="line">----</span><br><span class="line">china</span><br><span class="line">测试</span><br><span class="line">&gt; select * from <span class="string">"测试"</span></span><br><span class="line">name: 测试</span><br><span class="line">time                情况     数据点</span><br><span class="line">----                ------ ---------</span><br><span class="line">1587718700471996129 良好     桩1</span><br></pre></td></tr></table></figure><img src="https://raw.githubusercontent.com/SimonHu-HN/GoPic_Private/master/img/image-20200424175056166.png" alt="image-20200424175056166" style="zoom:67%;" /><h3 id="读取操作"><a href="#读取操作" class="headerlink" title="读取操作"></a>读取操作</h3><h4 id="SHOW-语句"><a href="#SHOW-语句" class="headerlink" title="SHOW 语句"></a>SHOW 语句</h4><table><thead><tr><th><a href="https://docs.influxdata.com/influxdb/v1.8/query_language/explore-schema/#show-databases" target="_blank" rel="noopener">SHOW DATABASES</a></th><th><a href="https://docs.influxdata.com/influxdb/v1.8/query_language/explore-schema/#show-retention-policies" target="_blank" rel="noopener">SHOW RETENTION POLICIES</a></th><th><a href="https://docs.influxdata.com/influxdb/v1.8/query_language/explore-schema/#show-series" target="_blank" rel="noopener">SHOW SERIES</a></th></tr></thead><tbody><tr><td><a href="https://docs.influxdata.com/influxdb/v1.8/query_language/explore-schema/#show-measurements" target="_blank" rel="noopener">SHOW MEASUREMENTS</a></td><td><a href="https://docs.influxdata.com/influxdb/v1.8/query_language/explore-schema/#show-tag-keys" target="_blank" rel="noopener">SHOW TAG KEYS</a></td><td><a href="https://docs.influxdata.com/influxdb/v1.8/query_language/explore-schema/#show-tag-values" target="_blank" rel="noopener">SHOW TAG VALUES</a></td></tr><tr><td><a href="https://docs.influxdata.com/influxdb/v1.8/query_language/explore-schema/#show-field-keys" target="_blank" rel="noopener">SHOW FIELD KEYS</a></td><td><a href="https://docs.influxdata.com/influxdb/v1.8/query_language/explore-schema/#filter-meta-queries-by-time" target="_blank" rel="noopener">Filter meta queries by time</a></td><td></td></tr></tbody></table><p>展示数据库中已存储的field的类型</p><p>The<code>SHOW FIELD KEYS</code>query also returns the field’s type.</p><p><strong>Example</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt; SHOW FIELD KEYS FROM all_the_types</span><br><span class="line">name: all_the_types</span><br><span class="line"><span class="comment">-------------------</span></span><br><span class="line">fieldKey  fieldType</span><br><span class="line">blue      string</span><br><span class="line">green     boolean</span><br><span class="line">orange    integer</span><br><span class="line">yellow    float</span><br></pre></td></tr></table></figure><h4 id="SELECT-语句"><a href="#SELECT-语句" class="headerlink" title="SELECT 语句"></a>SELECT 语句</h4><p>The <code>SELECT</code> statement queries data from a particular <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#measurement" target="_blank" rel="noopener">measurement</a> or measurements.</p><p>Syntax</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> &lt;field_key&gt;[,&lt;field_key&gt;,&lt;tag_key&gt;] <span class="keyword">FROM</span> &lt;measurement_name&gt;[,&lt;measurement_name&gt;]</span><br></pre></td></tr></table></figure><p>The <code>SELECT</code> statement requires a <code>SELECT</code> clause and a <code>FROM</code> clause.</p><p><code>SELECT</code> clause</p><p>The <code>SELECT</code> clause supports several formats for specifying data:</p><p><code>SELECT *</code>：Returns all <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#field" target="_blank" rel="noopener">fields</a> and <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#tag" target="_blank" rel="noopener">tags</a>.</p><p><code>SELECT &quot;&lt;field_key&gt;&quot;</code>：Returns a specific field.</p><p><code>SELECT &quot;&lt;field_key&gt;&quot;,&quot;&lt;field_key&gt;&quot;</code> ：Returns more than one field.</p><p><code>SELECT &quot;&lt;field_key&gt;&quot;,&quot;&lt;tag_key&gt;&quot;</code> ：Returns a specific field and tag. ：The <code>SELECT</code> clause must specify at least one field when it includes a tag.</p><p><code>SELECT &quot;&lt;field_key&gt;&quot;::field,&quot;&lt;tag_key&gt;&quot;::tag</code>：Returns a specific field and tag. The <code>::[field | tag]</code> syntax specifies the identifier’s type. Use this syntax to differentiate between field keys and tag keys that have the same name.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">实例：</span><br><span class="line">&gt; SELECT "level description"::field,"location"::tag,"water_level"::field FROM "h2o_feet"</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> *::<span class="keyword">field</span> <span class="keyword">from</span> <span class="string">"h2o_feet"</span> <span class="comment">/* 该查询从h2o_feet测量中选择所有字段。 SELECT子句支持将*语法与::语法结合使用。</span></span><br><span class="line"><span class="comment">然而，SELECT *::tag from "h2o_feet" 这种用法没有见过，也查不到东西</span></span><br><span class="line"><span class="comment">原因：The query’s SELECT clause must include at least one field key*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*That syntax is not required for most use cases.*/</span></span><br></pre></td></tr></table></figure><p><code>Select all data from a fully qualified measurement</code> select语句的完整路径（不使用USE）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt; SELECT * FROM "NOAA_water_database"."autogen"."h2o_feet"</span><br><span class="line"></span><br><span class="line">name: h2o_feet</span><br><span class="line"><span class="comment">--------------</span></span><br><span class="line">time                   level description      location       water_level</span><br><span class="line">2015-08-18T00:00:00Z   below 3 feet           santa_monica   2.064</span><br><span class="line">2015-08-18T00:00:00Z   between 6 and 9 feet   coyote_creek   8.12</span><br><span class="line">[...]</span><br><span class="line">2015-09-18T21:36:00Z   between 3 and 6 feet   santa_monica   5.066</span><br><span class="line">2015-09-18T21:42:00Z   between 3 and 6 feet   santa_monica   4.938</span><br></pre></td></tr></table></figure><p><code>Select all data from a measurement in a particular database</code> select语句查询特定数据库，使用默认RP</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; SELECT * FROM "NOAA_water_database".."h2o_feet"</span><br><span class="line"><span class="comment">/*The '..' indicates the DEFAULT retention (autogen) policy for the specified database.*/</span></span><br><span class="line"></span><br><span class="line">name: h2o_feet</span><br><span class="line"><span class="comment">--------------</span></span><br><span class="line">time                   level description      location       water_level</span><br><span class="line">2015-08-18T00:00:00Z   below 3 feet           santa_monica   2.064</span><br><span class="line">2015-08-18T00:00:00Z   between 6 and 9 feet   coyote_creek   8.12</span><br><span class="line">[...]</span><br><span class="line">2015-09-18T21:36:00Z   between 3 and 6 feet   santa_monica   5.066</span><br><span class="line">2015-09-18T21:42:00Z   between 3 and 6 feet   santa_monica   4.938</span><br></pre></td></tr></table></figure><p><strong>常见SELECT遇到的问题</strong> Common issues with the SELECT statement</p><p>Selecting tag keys in the SELECT clause</p><p>A query requires at least one <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#field-key" target="_blank" rel="noopener">field key</a> in the <code>SELECT</code> clause to return data. If the <code>SELECT</code> clause only includes a single <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#tag-key" target="_blank" rel="noopener">tag key</a> or several tag keys, the query returns an empty response. This behavior is a result of how the system stores data.</p><p>Example</p><p>The following query returns no data because it specifies a single tag key (<code>location</code>) in the <code>SELECT</code> clause:</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; SELECT "location" FROM "h2o_feet"</span><br></pre></td></tr></table></figure><p>To return any data associated with the <code>location</code> tag key, the query’s <code>SELECT</code> clause must include at least one field key (<code>water_level</code>):</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; SELECT "water_level","location" FROM "h2o_feet"</span><br><span class="line">name: h2o_feet</span><br><span class="line">time                   water_level  location</span><br><span class="line"><span class="comment">----                   -----------  --------</span></span><br><span class="line">2015-08-18T00:00:00Z   8.12         coyote_creek</span><br><span class="line">2015-08-18T00:00:00Z   2.064        santa_monica</span><br><span class="line">[...]</span><br><span class="line">2015-09-18T21:36:00Z   5.066        santa_monica</span><br><span class="line">2015-09-18T21:42:00Z   4.938        santa_monica</span><br></pre></td></tr></table></figure><h4 id="From-语句"><a href="#From-语句" class="headerlink" title="From 语句"></a>From 语句</h4><p>The <code>FROM</code> clause supports several formats for specifying a <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#measurement" target="_blank" rel="noopener">measurement(s)</a>:</p><p><code>FROM &lt;measurement_name&gt;</code>      Returns data from a single measurement. If you’re using the <a href="https://docs.influxdata.com/influxdb/v1.8/tools/shell/" target="_blank" rel="noopener">CLI</a> InfluxDB queries the measurement in the <a href="https://docs.influxdata.com/influxdb/v1.8/tools/shell/#commands" target="_blank" rel="noopener"><code>USE</code>d</a> <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#database" target="_blank" rel="noopener">database</a> and the <code>DEFAULT</code> <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#retention-policy-rp" target="_blank" rel="noopener">retention policy</a>. If you’re using the <a href="https://docs.influxdata.com/influxdb/v1.8/tools/api/" target="_blank" rel="noopener">InfluxDB API</a> InfluxDB queries the measurement in the database specified in the <a href="https://docs.influxdata.com/influxdb/v1.8/tools/api/#query-string-parameters" target="_blank" rel="noopener"><code>db</code> query string parameter</a> and the <code>DEFAULT</code> retention policy.</p><p><code>FROM &lt;measurement_name&gt;,&lt;measurement_name&gt;</code>      Returns data from more than one measurement.</p><p><code>FROM &lt;database_name&gt;.&lt;retention_policy_name&gt;.&lt;measurement_name&gt;</code>      Returns data from a fully qualified measurement. Fully qualify a measurement by specifying its database and retention policy.</p><p><code>FROM &lt;database_name&gt;..&lt;measurement_name&gt;</code>      Returns data from a measurement in a user-specified <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#database" target="_blank" rel="noopener">database</a> and the <code>DEFAULT</code> <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#retention-policy-rp" target="_blank" rel="noopener">retention policy</a>.</p><p>Other supported features: <a href="https://docs.influxdata.com/influxdb/v1.8/query_language/explore-data/#regular-expressions" target="_blank" rel="noopener">Regular Expressions</a></p><h4 id="Where-语句"><a href="#Where-语句" class="headerlink" title="Where 语句"></a>Where 语句</h4><p>The <code>WHERE</code> filters data based on <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#field" target="_blank" rel="noopener">fields</a>, <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#tag" target="_blank" rel="noopener">tags</a>, and/or <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#timestamp" target="_blank" rel="noopener">timestamps</a>.</p><p>示例：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; SELECT * FROM "absolutismus" WHERE time = '2016-07-31T20:07:00Z' OR time = '2016-07-31T23:07:17Z'</span><br></pre></td></tr></table></figure><p><strong>Fields处条件判断</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">field_key &lt;operator&gt; [&#39;string&#39; | boolean | float | integer]</span><br><span class="line">&#x2F;* field_key 双引号可加可不加，单引号不会返回东西 *&#x2F;</span><br></pre></td></tr></table></figure><p><strong>Tags处条件判断</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tag_key &lt;operator&gt; ['tag_value']</span><br><span class="line"><span class="comment">/* tag_key 双引号可加可不加，单引号不会返回东西, value一定要加单引号*/</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt; SELECT "water_level" FROM "h2o_feet" WHERE "location" = santa_monica /*ERROR*/</span><br><span class="line"></span><br><span class="line">&gt; SELECT "water_level" FROM "h2o_feet" WHERE "location" = "santa_monica" /*ERROR*/</span><br><span class="line"></span><br><span class="line">&gt; SELECT "water_level" FROM "h2o_feet" WHERE "location" = 'santa_monica' /*CORRECT*/</span><br><span class="line"></span><br><span class="line">name: h2o_feet</span><br><span class="line"><span class="comment">--------------</span></span><br><span class="line">time                   water_level</span><br><span class="line">2015-08-18T00:00:00Z   2.064</span><br><span class="line">[...]</span><br><span class="line">2015-09-18T21:42:00Z   4.938</span><br></pre></td></tr></table></figure><p><strong>Timestamps</strong></p><p>For most <code>SELECT</code> statements, the default time range is between <a href="https://docs.influxdata.com/influxdb/v1.8/troubleshooting/frequently-asked-questions/#what-are-the-minimum-and-maximum-timestamps-that-influxdb-can-store" target="_blank" rel="noopener"><code>1677-09-21 00:12:43.145224194 and 2262-04-11T23:47:16.854775806Z</code> UTC</a>. For <code>SELECT</code> statements with a <a href="https://docs.influxdata.com/influxdb/v1.8/query_language/explore-data/#group-by-time-intervals" target="_blank" rel="noopener"><code>GROUP BY time()</code> clause</a>, the default time range is between <code>1677-09-21 00:12:43.145224194</code> UTC and <a href="https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#now" target="_blank" rel="noopener"><code>now()</code></a>.</p><h2 id="生产环境注意事项及相关实例"><a href="#生产环境注意事项及相关实例" class="headerlink" title="生产环境注意事项及相关实例"></a>生产环境注意事项及相关实例</h2><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p><code>注意:由于influxdb官方没有对进程进行内存限制，因此在很容易造成OOM，建议在使用前提前注意以下几点</code></p><ul><li><p>1.索引类型切换为<code>tsi1</code>。默认的索引类型为<code>inmem</code>，当<code>series cardinality</code>超过4百万的时候，很容易造成OOM。<code>tsi1</code>使用冷热数据分离的方式，可以很容易支持上千万的<code>series</code></p><ol><li><p>To enable TSI in InfluxDB 1.8.x, complete the following steps:</p><p>a. If using the InfluxDB configuration file, find the <code>[data]</code> section, uncomment <code>index-version = &quot;inmem&quot;</code> and change the value to <code>tsi1</code>.</p><figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- ./etc/config.sample.toml --&gt;</span><br><span class="line"></span><br><span class="line"><span class="section">[data]</span></span><br><span class="line">  <span class="comment"># The directory where the TSM storage engine stores TSM files.</span></span><br><span class="line">  dir = "/var/lib/influxdb/data"</span><br><span class="line"></span><br><span class="line">  <span class="comment"># The directory where the TSM storage engine stores WAL files.</span></span><br><span class="line">  wal-dir = "/var/lib/influxdb/wal"</span><br><span class="line"></span><br><span class="line">  <span class="comment"># The amount of time that a write will wait before fsyncing.  A duration</span></span><br><span class="line">  <span class="comment"># greater than 0 can be used to batch up multiple fsync calls.  This is useful for slower</span></span><br><span class="line">  <span class="comment"># disks or when WAL write contention is seen.  A value of 0s fsyncs every write to the WAL.</span></span><br><span class="line">  <span class="comment"># Values in the range of 0-100ms are recommended for non-SSD disks.</span></span><br><span class="line">  <span class="comment"># wal-fsync-delay = "0s"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># The type of shard index to use for new shards.  The default is an in-memory index that is</span></span><br><span class="line">  <span class="comment"># recreated at startup.  A value of "tsi1" will use a disk based index that supports higher</span></span><br><span class="line">  <span class="comment"># cardinality datasets.</span></span><br><span class="line">  <span class="comment"># index-version = "inmem"</span></span><br></pre></td></tr></table></figure><p>b. If using environment variables, set <code>INFLUXDB_DATA_INDEX_VERSION</code> to <code>tsi1</code>.</p><p>c. Delete shard <code>index</code> directories (by default, located at <code>//index</code>).</p><p>d. Convert TSM-based shards to TSI-based shards by running the <a href="https://docs.influxdata.com/influxdb/v1.8/tools/influx_inspect/#buildtsi" target="_blank" rel="noopener">influx_inspect buildtsi</a> command.</p><blockquote><p><strong>Note</strong> Run the buildtsi command using the user account that you are going to run the database as, or ensure that the permissions match afterward.</p></blockquote></li></ol></li></ul><ol start="2"><li>Restart the <code>influxdb</code> service.</li></ol><ul><li><p>2.尽量将InfluxDB的数据存储在<code>SSD</code>之类的高性能存储上</p></li><li><p>3.虽然NoSQL存储使用较为方便，<code>schema</code>设计仍然要注意</p></li><li><p>4.合理的利用<code>continuous query</code>和<code>retention policy</code>来对历史数据进行归档以及删除</p></li></ul><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p>饿了么：</p><p><a href="https://github.com/pingliu/influxdb-gateway" target="_blank" rel="noopener">influxdb-gateway</a> : 用于检测和压缩influxdb的数据，用于跨机房传输</p><p><a href="https://github.com/influxdata/influxdb-relay" target="_blank" rel="noopener">influxdb-relay</a> : 官方提供的高可用方案，但是它只提供简单的写入功能。</p><p><a href="https://github.com/shell909090/influx-proxy" target="_blank" rel="noopener">influx-proxy</a> : 是用于替代 influxdb-relay 的高可用方案。</p>]]></content>
      
      
      <categories>
          
          <category> Test </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Test </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
